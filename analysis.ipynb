{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Volatility \n",
    "\n",
    "#### David Montoto\n",
    "#### Vatsal Bagri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/opt/python@3.12/libexec/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/opt/python@3.12/libexec/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1 - Stock data from Yahoo Finance\n",
    "\n",
    "Our initial dataset, a collection of stocks along with their varying prices and dividends with the given data, are provided from Yahoo Finance. Most of the data is easy to work with as it has been polished. However, the 'Date' category can be simplified to just the provided date not including time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/opt/python@3.12/libexec/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/opt/python@3.12/libexec/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df1_initial = pd.read_csv('data/stock_details_5_years.csv', header = 0)\n",
    "df1_initial.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/opt/python@3.12/libexec/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/opt/python@3.12/libexec/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df1_initial['Date'] = pd.to_datetime(df1_initial['Date'].str.extract(r'(\\d{4}-\\d{2}-\\d{2})')[0])\n",
    "df1_initial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 2 - Volatility Data For Each Company (Computed and Wrangled From Data set 1)\n",
    "\n",
    "We will create a second dataset. This dataset will be taken from a subset of the date from out first created dataset. We aim to group different stocks by volatility. This data is grouped seperately because it will be used seperately from the rest of the data in dataset 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/opt/python@3.12/libexec/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/opt/python@3.12/libexec/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df1_initial['perc_change'] = abs(((df1_initial['Close'] - df1_initial['Open']) / df1_initial['Open']) * 100.0)\n",
    "\n",
    "changes_df = df1_initial.groupby('Company')['perc_change'].apply(list).reset_index()\n",
    "\n",
    "changes_df['length'] = changes_df['perc_change'].apply(len)\n",
    "full_length = changes_df['length'].max()\n",
    "final_df = changes_df[changes_df['length'] == full_length].drop(columns=['length'])\n",
    "\n",
    "final_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/opt/python@3.12/libexec/bin/python' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/opt/python@3.12/libexec/bin/python -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "X = np.array(final_df['perc_change'].tolist())\n",
    "pca = PCA(n_components=250) \n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=11, random_state=24) # use of 11 explained in writing above\n",
    "cluster_labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "silh_score = silhouette_score(X_pca, cluster_labels)\n",
    "print('silhouette score:', silh_score)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='hsv', alpha=0.6)\n",
    "plt.title('Baseline k-means clustering')\n",
    "plt.xlabel('principal component x direction')\n",
    "plt.ylabel('principal component y direction')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Analysis\n",
    "\n",
    "In our stock market analysis, we used Principal Component Analysis (PCA) to reduce the complexity of our high-dimensional dataset, where each companyâ€™s stock data had 1250 observations. To determine the optimal number of components, we used the elbow method and found that 84 components explained about 95% of the variance, balancing dimensionality reduction and data integrity. We also addressed a data cleaning issue by excluding companies with fewer than 1250 observations, ensuring consistency for PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(final_df['perc_change'].tolist())\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "# elbow method shown here\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Components')\n",
    "plt.ylabel('Variance')\n",
    "plt.title('Variance vs. total components')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following subsection we will conduct clustering of our data, cross validation for hyper parameter tuning, analysis of different final clusters and supervised learning analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline K Means and GMM Models\n",
    "\n",
    "To compare the performance of the GMM and K-means models on our dataset, we first implemented both models using 11 clusters, corresponding to the 11 industries in the S&P 500, to explore potential industry patterns. We began by establishing a baseline, running both models on PCA-processed data without any hyperparameter tuning. To evaluate the models, we visualized the results through graphs and calculated the silhouette scores for each model. This allowed us to assess how well the models performed in clustering the data and provided insight into their overall effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(final_df['perc_change'].tolist())\n",
    "pca = PCA(n_components=84) \n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=11, random_state=24) # use of 11 explained in writing above\n",
    "cluster_labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "silh_score = silhouette_score(X_pca, cluster_labels)\n",
    "print('silhouette score:', silh_score)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='hsv', alpha=0.6)\n",
    "plt.title('Baseline k-means clustering')\n",
    "plt.xlabel('principal component x direction')\n",
    "plt.ylabel('principal component y direction')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=11, random_state=24)  \n",
    "cluster_labels = gmm.fit_predict(X_pca)\n",
    "\n",
    "silh_score = silhouette_score(X_pca, cluster_labels)\n",
    "print('silhouette score:', silh_score)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='hsv', alpha=0.6)\n",
    "plt.title('Baseline GMM clustering')\n",
    "plt.xlabel('principal component x direction')\n",
    "plt.ylabel('principal component y direction')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Cluster Assignment between the two techiniques \n",
    "\n",
    "Due to the similarity in the silhouette scores of the two techiques, we printed out the clusters which were created just to ensure the validity of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels = kmeans.predict(X_pca)\n",
    "gmm_labels = gmm.predict(X_pca)\n",
    "\n",
    "# Compare the cluster assignments\n",
    "print(\"KMeans cluster assignments:\", kmeans_labels[:10])\n",
    "print(\"GMM cluster assignments:\", gmm_labels[:10])\n",
    "\n",
    "kmeans_silhouette = silhouette_score(X_pca, kmeans_labels)\n",
    "gmm_silhouette = silhouette_score(X_pca, gmm_labels)\n",
    "\n",
    "print(f\"KMeans Silhouette Score: {kmeans_silhouette}\")\n",
    "print(f\"GMM Silhouette Score: {gmm_silhouette}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation and optimization of Baseline GMM and K Means\n",
    "To optimize our GMM and K-means models for clustering stocks, we used cross-validation to determine the ideal number of clusters. This process tested the models on different data segments, providing performance scores and ensuring robustness. Cross-validation helped us identify the optimal number of clusters that maximized coherence and distinction, which we assessed using the silhouette score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "\n",
    "# corss validation here, note it is unseeded\n",
    "for n_clusters in range(2, 19):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    cluster_labels = kmeans.fit_predict(X_pca)\n",
    "    silh_score = silhouette_score(X_pca, cluster_labels)\n",
    "    silhouette_scores += [silh_score]\n",
    "\n",
    "final_cluster_num = np.argmax(silhouette_scores) + 2 # add 2 because corresponding index is left shifted\n",
    "\n",
    "print(\"best # of clusters:\", final_cluster_num)\n",
    "print(\"Corresponding best cross validation silhouette score\", silhouette_scores[final_cluster_num - 2])\n",
    "\n",
    "#kmeans used new found best k param\n",
    "kmeans = KMeans(n_clusters=final_cluster_num, random_state=24)\n",
    "cluster_labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "#saving labl results for later analysis in finl-df\n",
    "final_df['k-means label'] = cluster_labels\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='coolwarm', alpha=0.5)\n",
    "plt.title('Tuned k-means Lables')\n",
    "plt.xlabel('Principal Component x direction')\n",
    "plt.ylabel('Principal Component y direction')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# silhoutte score from graphed kmeans clustering\n",
    "final_silhouette_score = silhouette_score(X_pca, cluster_labels)\n",
    "print(\"Silhouette score for graphed clustered:\", final_silhouette_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "\n",
    "# cross validation here, note this part is unseeded\n",
    "for n_components in range(2, 19):\n",
    "    gmm = GaussianMixture(n_components=n_components)\n",
    "    cluster_labels = gmm.fit_predict(X_pca)\n",
    "    silh_score = silhouette_score(X_pca, cluster_labels)\n",
    "    silhouette_scores.append(silh_score)\n",
    "\n",
    "final_cluster_num = np.argmax(silhouette_scores) + 2\n",
    "\n",
    "print(\"best # of clusters:\", final_cluster_num)\n",
    "print(\"Corresponding best cross validation silhouette score\", silhouette_scores[final_cluster_num - 2])\n",
    "\n",
    "gmm = GaussianMixture(n_components=final_cluster_num, random_state=24)\n",
    "cluster_labels = gmm.fit_predict(X_pca)\n",
    "\n",
    "# Update final_df with cluster labels\n",
    "final_df['gmm labels'] = cluster_labels\n",
    "\n",
    "# Plot the clustering labels\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='coolwarm', alpha=0.5)\n",
    "plt.title('Tuned gmm Lables')\n",
    "plt.xlabel('Principal Component x direction')\n",
    "plt.ylabel('Principal Component y direction')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "final_silhouette_score = silhouette_score(X_pca, cluster_labels)\n",
    "print(\"Silhouette score for graphed clustered:\", final_silhouette_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Clusters from Each Model\n",
    "\n",
    "After improving the models, both GMM and K-means models repeeatedly selected 2 clusters in the cross validation. Since groups are now assigned to companies from the models, we will proceed with a performance analysis to learn how these clusters and models differ from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak at data frame after adding labels from our models\n",
    "final_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volatility comparision of Clusters in K-means Model\n",
    "final_df['avg_volatility'] = final_df['perc_change'].apply(lambda x: x.mean())\n",
    "final_df[['k-means label', 'avg_volatility']].groupby('k-means label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volatility comparision of Clusters in GMM Model\n",
    "final_df[['gmm labels', 'avg_volatility']].groupby('gmm labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_0 = final_df.index[final_df['k-means label'] == 0].values\n",
    "\n",
    "kmeans_1 = final_df.index[final_df['k-means label'] == 1].values\n",
    "\n",
    "gmm_0 = final_df.index[final_df['gmm labels'] == 0].values\n",
    "\n",
    "gmm_1 = final_df.index[final_df['gmm labels'] == 1].values\n",
    "\n",
    "#lengths of each \n",
    "\n",
    "print(\"k-means 0 length:\", len(kmeans_0))\n",
    "print(\"k-means 1 length:\", len(kmeans_1), '\\n')\n",
    "print(\"gmm 0 length:\", len(gmm_0))\n",
    "print(\"gmm 1 length:\", len(gmm_1), '\\n')\n",
    "\n",
    "# to visualize what these arrays look like here is a snippet of first 10 companies in the 1-label groups\n",
    "print(gmm_1[0:10])\n",
    "print(kmeans_1[0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
